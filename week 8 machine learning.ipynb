{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce045271-ab6e-4538-a079-6cfee4a12972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "# Assume X_train, X_test, y_train, y_test are already defined and preprocessed\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Try different thresholds\n",
    "thresholds = [0.3, 0.5, 0.7]\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_probs >= thresh).astype(int)\n",
    "    acc = accuracy_score(y_test, y_pred_thresh)\n",
    "    prec = precision_score(y_test, y_pred_thresh)\n",
    "    rec = recall_score(y_test, y_pred_thresh)\n",
    "    print(f\"Threshold {thresh}: Accuracy={acc:.2f}, Precision={prec:.2f}, Recall={rec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9aa240-4e93-4932-ab17-b6cc0055570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_columns = ['target1', 'target2']  # example targets\n",
    "for col in target_columns:\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train[col])\n",
    "    y_probs = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        y_pred = (y_probs >= thresh).astype(int)\n",
    "        acc = accuracy_score(y_test[col], y_pred)\n",
    "        print(f\"{col} - Threshold {thresh}: Accuracy={acc:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66996171-aec2-429f-8522-ed859dc4c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define feature types\n",
    "numeric_features = ['num1', 'num2']\n",
    "categorical_features = ['cat1', 'cat2']\n",
    "\n",
    "# Preprocessing steps\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numeric_features),\n",
    "    ('cat', OneHotEncoder(), categorical_features)\n",
    "])\n",
    "\n",
    "# Create pipeline\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Train model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879f6da7-aaab-4caf-93ea-2c8919f195ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_probs = clf.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "auc_score = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "plt.plot(fpr, tpr, label=f'LogReg AUC={auc_score:.2f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # random line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe2093-67cd-431c-8231-b6de1b3a4d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume your data is in DataFrame `X`\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "K = range(2, 11)\n",
    "\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouettes.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "\n",
    "# Plot inertia and silhouette score\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(K, inertias, marker='o')\n",
    "plt.title('Inertia vs k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Inertia')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(K, silhouettes, marker='o')\n",
    "plt.title('Silhouette Score vs k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46864a3f-7cc9-42a7-a9e5-0b0c62cc6539",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. What if you don't scale your features?\n",
    "\n",
    "If features are not scaled, those with larger numeric ranges will dominate the distance calculations used in clustering algorithms like KMeans. This leads to:\n",
    "\n",
    "- **Biased clustering results**: One feature may disproportionately influence cluster assignment.\n",
    "- **Poor Silhouette and Inertia scores**: Because clusters are distorted and not representative of true groupings.\n",
    "- **Incorrect conclusions**: Features with smaller ranges are undervalued in the clustering process.\n",
    "\n",
    "#### Example Comparison (With vs Without Scaling)\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# With scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "kmeans_scaled = KMeans(n_clusters=3, random_state=42).fit(X_scaled)\n",
    "score_scaled = silhouette_score(X_scaled, kmeans_scaled.labels_)\n",
    "\n",
    "# Without scaling\n",
    "kmeans_raw = KMeans(n_clusters=3, random_state=42).fit(X)\n",
    "score_raw = silhouette_score(X, kmeans_raw.labels_)\n",
    "\n",
    "print(f\"Silhouette Score with scaling: {score_scaled:.2f}\")\n",
    "print(f\"Silhouette Score without scaling: {score_raw:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ade28-c2d8-4163-bfff-8c7c2550c078",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 3: Is there a 'right' k? Why or why not?\n",
    "\n",
    "In KMeans clustering, choosing the \"right\" number of clusters (`k`) is not always straightforward. There is **no universally correct value of k**, but you can use several techniques to estimate a suitable choice.\n",
    "\n",
    "### üîç Common Methods to Choose k\n",
    "\n",
    "1. **Elbow Method**:\n",
    "   - Plot the inertia (within-cluster sum of squares) against different values of k.\n",
    "   - Look for the \"elbow\" point where the decrease in inertia slows down ‚Äî this suggests diminishing returns with higher k.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.cluster import KMeans\n",
    "   import matplotlib.pyplot as plt\n",
    "\n",
    "   inertias = []\n",
    "   K = range(1, 11)\n",
    "\n",
    "   for k in K:\n",
    "       kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "       kmeans.fit(X_scaled)\n",
    "       inertias.append(kmeans.inertia_)\n",
    "\n",
    "   plt.plot(K, inertias, marker='o')\n",
    "   plt.title(\"Elbow Method: Inertia vs K\")\n",
    "   plt.xlabel(\"Number of Clusters (k)\")\n",
    "   plt.ylabel(\"Inertia\")\n",
    "   plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c00ed6c-02e9-4338-8a22-d1a932b51f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset (replace with your actual file path)\n",
    "df = pd.read_csv(\"food_nutrients.csv\")\n",
    "X = df[['Protein', 'Fat', 'Carbohydrate', 'Calories']]  # example features\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Test different k values\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "k_range = range(2, 10)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouettes.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_range, inertias, marker='o')\n",
    "plt.title(\"Inertia vs K\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_range, silhouettes, marker='o')\n",
    "plt.title(\"Silhouette Score vs K\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
